{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturação do **GRADIENTE**\n",
    "\n",
    "Geralmente ocorre em funções de ativação não lineares como Sigmoid e Tangente Hiperbólica (tanh), acontece quando os gradientes se tornam muito pequenos, dificultando o aprendizado da rede\n",
    "\n",
    "✅ Para evitar saturação:\n",
    "\n",
    "- Prefira ReLU, Leaky ReLU ou ELU em vez de sigmoide/tanh.\n",
    "\n",
    "- Use Batch Normalization, que mantém os valores das ativações dentro de uma faixa saudável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuga do **GRADIENTE**\n",
    "\n",
    "Ocorre quando os gradientes se tornam muito grandes, fazendo com que os pesos explodam e a rede fique instável. Esse problema é comum em redes muito profundas, principalmente com funções de ativação que não possuem limite superior. pode acontecer em **Redes Recorrentes**, **Variantes da ReLU (Leaky ReLU, PReLU, ELU)**, funções que não possuem **limite superior para os valores da ativação**.\n",
    "\n",
    "✅ Para evitar fuga do gradiente:\n",
    "\n",
    "- Use Inicialização de Pesos adequada (ex: He Initialization para ReLU).\n",
    "\n",
    "- Utilize Batch Normalization para manter os gradientes sob controle.\n",
    "\n",
    "- Gradiente Clipping: Limita o tamanho do gradiente para evitar explosão.\n",
    "\n",
    "- Regularização L2 (Weight Decay): Penaliza pesos muito grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeakyRelu\n",
    "\n",
    "Utilizando a função de ativação *RelU* (onde o valor de entrada positivo permanece o mesmo, enquanto uma entrada negativa se torna zero), pode ser que o nosso modelo mate alguns neurônios pois se um neurônio recebe valores negativos continuamente, sempre retornará zero. A *LeakyRelu* visa consertar isso. Em sua função, ao invés de zerar valores de entrada negativa, ela os transforma em um fator α pequeno, geralmente α = 0.01, fazendo assim que em algum momento aquele neurônio \"volte a vida\", aprendendo novamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 Como escolher a função de ativação da última camada?\n",
    "\n",
    "| **Tipo de Problema**                                          | **Função de Ativação na Saída**         | **Explicação** |\n",
    "|--------------------------------------------------------------|---------------------------------|--------------|\n",
    "| **Classificação binária** (2 classes: \"sim/não\", \"gato/cachorro\") | `sigmoid`                     | Retorna um valor entre **0 e 1**, interpretado como **probabilidade**. |\n",
    "| **Classificação multiclasse** (3 ou mais classes, previsão de uma única classe) | `softmax`                     | Converte os logits em **probabilidades normalizadas** (somam 1), útil para **1 classe por amostra**. |\n",
    "| **Classificação multirrótulo** (cada amostra pode ter várias classes) | `sigmoid`                      | Cada saída pode ser ativada independentemente, pois cada rótulo é tratado separadamente. |\n",
    "| **Regressão** (previsão de valores contínuos, como temperatura, preço de casa) | `linear` (ou nenhuma ativação) | Permite qualquer valor real como saída, sem restrições. |\n",
    "| **Regressão para valores positivos** (exemplo: previsão de contagem) | `relu` ou `softplus`          | Garante que a saída seja **sempre positiva**. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\OneDrive\\Desktop\\RedesNeurais-Basico\\environment\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\anton\\OneDrive\\Desktop\\RedesNeurais-Basico\\environment\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), # Tamanho das imagens\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2), # pode ser \"PReLU()\" no lugar da LeakyReLU, mas se quiser usar SELU precisa definir \"activation='selu'\" e \"kernel_initializer='cun_normal'\"\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax') # 10 pois são 10 classes possíveis de saída. (classes das fotos do MNIST)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4755 - loss: 1.6578 - val_accuracy: 0.7168 - val_loss: 0.8749\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7209 - loss: 0.8360 - val_accuracy: 0.7734 - val_loss: 0.7050\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7734 - loss: 0.6927 - val_accuracy: 0.8020 - val_loss: 0.6257\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7959 - loss: 0.6230 - val_accuracy: 0.8136 - val_loss: 0.5801\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8076 - loss: 0.5804 - val_accuracy: 0.8222 - val_loss: 0.5515\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8149 - loss: 0.5552 - val_accuracy: 0.8302 - val_loss: 0.5246\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8195 - loss: 0.5360 - val_accuracy: 0.8314 - val_loss: 0.5095\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.5122 - val_accuracy: 0.8360 - val_loss: 0.4960\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.5060 - val_accuracy: 0.8394 - val_loss: 0.4821\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8328 - loss: 0.4938 - val_accuracy: 0.8420 - val_loss: 0.4740\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8345 - loss: 0.4818 - val_accuracy: 0.8446 - val_loss: 0.4662\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8355 - loss: 0.4736 - val_accuracy: 0.8452 - val_loss: 0.4592\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8405 - loss: 0.4657 - val_accuracy: 0.8478 - val_loss: 0.4525\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8436 - loss: 0.4584 - val_accuracy: 0.8476 - val_loss: 0.4488\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.4510 - val_accuracy: 0.8480 - val_loss: 0.4483\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8458 - loss: 0.4438 - val_accuracy: 0.8538 - val_loss: 0.4377\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8436 - loss: 0.4483 - val_accuracy: 0.8508 - val_loss: 0.4393\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8500 - loss: 0.4378 - val_accuracy: 0.8518 - val_loss: 0.4326\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8508 - loss: 0.4358 - val_accuracy: 0.8540 - val_loss: 0.4281\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8501 - loss: 0.4321 - val_accuracy: 0.8552 - val_loss: 0.4291\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8502 - loss: 0.4329 - val_accuracy: 0.8556 - val_loss: 0.4235\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8525 - loss: 0.4257 - val_accuracy: 0.8582 - val_loss: 0.4206\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8538 - loss: 0.4224 - val_accuracy: 0.8580 - val_loss: 0.4190\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8551 - loss: 0.4182 - val_accuracy: 0.8576 - val_loss: 0.4183\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8549 - loss: 0.4141 - val_accuracy: 0.8582 - val_loss: 0.4135\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8569 - loss: 0.4139 - val_accuracy: 0.8602 - val_loss: 0.4101\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8569 - loss: 0.4119 - val_accuracy: 0.8598 - val_loss: 0.4091\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8601 - loss: 0.4036 - val_accuracy: 0.8620 - val_loss: 0.4055\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8646 - loss: 0.3963 - val_accuracy: 0.8600 - val_loss: 0.4088\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8604 - loss: 0.4017 - val_accuracy: 0.8614 - val_loss: 0.4022\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ Piora no modelo\n",
    "\n",
    "Por que será que o modelo utilizando o leakyRelU piorou em relação ao que fizemos no arquivo \"RedesSequenciais.ipynb\" utilizando uma rede sequencial simples?\n",
    "\n",
    "\n",
    "\n",
    "Fator |\tModelo com LeakyReLU (pior desempenho) |\tModelo com ReLU (melhor desempenho)\n",
    "|------------------------------------------------------|---------------------------------|--------------|\n",
    "Função de ativação |\tLeakyReLU(α=0.2) (mantém gradientes negativos, pode atrapalhar) |\tReLU (aprendizado mais rápido e eficiente)\n",
    "Inicialização |\the_normal (pode gerar pesos grandes demais) |\tglorot_uniform (mais equilibrado para redes pequenas)\n",
    "Complexidade |\tMais camadas e inicializações desnecessárias |\tModelo mais simples e eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎲 batch Normalization\n",
    "\n",
    "Utilizada para **normalizar** as **ativações das camadas intermediárias** de uma rede neural, o **BathNormalization** acelera o treinamento, melhora a performance e reduz a sensibilidade a inicialização dos pesos.\n",
    "\n",
    "Quando treinamos uma rede neural, as ativações das camadas podem ficar em diferentes escalas, o que pode dificultar a cnovergência durante o treinamento, já que o gradiente pode se tornar muito grande ou muito pequeno. Isso é conhecido como **covariate shift** (Mudança na distribuição das ativações das camadas). O **BatchNormalization** garante que as ativações tenham distribuição com média **0** e desvio padrão **1** durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3) ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6035 - loss: 1.1884 - val_accuracy: 0.7980 - val_loss: 0.5868\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7859 - loss: 0.6183 - val_accuracy: 0.8260 - val_loss: 0.5099\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8117 - loss: 0.5461 - val_accuracy: 0.8372 - val_loss: 0.4738\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8198 - loss: 0.5173 - val_accuracy: 0.8464 - val_loss: 0.4516\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.4956 - val_accuracy: 0.8516 - val_loss: 0.4364\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8350 - loss: 0.4687 - val_accuracy: 0.8544 - val_loss: 0.4251\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8355 - loss: 0.4645 - val_accuracy: 0.8582 - val_loss: 0.4155\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8410 - loss: 0.4557 - val_accuracy: 0.8600 - val_loss: 0.4089\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8410 - loss: 0.4507 - val_accuracy: 0.8642 - val_loss: 0.4024\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8475 - loss: 0.4291 - val_accuracy: 0.8666 - val_loss: 0.3972\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8478 - loss: 0.4320 - val_accuracy: 0.8650 - val_loss: 0.3936\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8551 - loss: 0.4101 - val_accuracy: 0.8666 - val_loss: 0.3881\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 0.4108 - val_accuracy: 0.8686 - val_loss: 0.3854\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8584 - loss: 0.3999 - val_accuracy: 0.8696 - val_loss: 0.3815\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8600 - loss: 0.3977 - val_accuracy: 0.8690 - val_loss: 0.3776\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8599 - loss: 0.3959 - val_accuracy: 0.8698 - val_loss: 0.3773\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8573 - loss: 0.3952 - val_accuracy: 0.8702 - val_loss: 0.3735\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8619 - loss: 0.3854 - val_accuracy: 0.8694 - val_loss: 0.3714\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8661 - loss: 0.3810 - val_accuracy: 0.8734 - val_loss: 0.3678\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8661 - loss: 0.3755 - val_accuracy: 0.8732 - val_loss: 0.3681\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8663 - loss: 0.3782 - val_accuracy: 0.8746 - val_loss: 0.3662\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8636 - loss: 0.3797 - val_accuracy: 0.8734 - val_loss: 0.3620\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8685 - loss: 0.3656 - val_accuracy: 0.8738 - val_loss: 0.3616\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8718 - loss: 0.3629 - val_accuracy: 0.8730 - val_loss: 0.3594\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8694 - loss: 0.3650 - val_accuracy: 0.8742 - val_loss: 0.3585\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8727 - loss: 0.3553 - val_accuracy: 0.8760 - val_loss: 0.3575\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8715 - loss: 0.3599 - val_accuracy: 0.8752 - val_loss: 0.3561\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8733 - loss: 0.3530 - val_accuracy: 0.8772 - val_loss: 0.3537\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8766 - loss: 0.3511 - val_accuracy: 0.8764 - val_loss: 0.3518\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8762 - loss: 0.3464 - val_accuracy: 0.8768 - val_loss: 0.3525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27b87be3a40>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez, houve uma piora pois o problema é \"simples\" e não requer um aprendizado tão profundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📥Dropout\n",
    "\n",
    "Um dos reguladores mais comuns nas redes neurais profunda são as camadas de *Dropout*, as redes neurais mais modernas podem obter um aumento de 1 a 2% de acurácia simplesmente por adicionar camadas de *Dropout* segundo umartigo de 2014 de **Nitish Srivastava**.\n",
    "\n",
    "Como o Dropout funciona?\n",
    "Durante o treinamento de uma rede neural, o Dropout \"desativa\" aleatoriamente um conjunto de neurônios em cada iteração, de maneira que eles não participam da ativação nem do cálculo do gradiente. Esse processo é aplicado de forma estocástica, ou seja, em cada atualização do modelo, diferentes neurônios são desativados.\n",
    "\n",
    "Por exemplo, se aplicarmos um Dropout de 50%, metade dos neurônios serão desativados aleatoriamente em cada ciclo de treinamento. Isso força a rede a aprender representações mais robustas e a não depender excessivamente de neurônios específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "logdir = os.path.join(os.curdir, \"ArquivosGeradosPelosCodigos/logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "# Criando TransBoard como callbackk\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3) ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.3702 - loss: 1.8105 - val_accuracy: 0.7122 - val_loss: 0.8202\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6537 - loss: 0.9512 - val_accuracy: 0.7596 - val_loss: 0.6885\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7061 - loss: 0.8158 - val_accuracy: 0.7844 - val_loss: 0.6284\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7331 - loss: 0.7476 - val_accuracy: 0.7982 - val_loss: 0.5920\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7455 - loss: 0.7089 - val_accuracy: 0.8066 - val_loss: 0.5654\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7564 - loss: 0.6778 - val_accuracy: 0.8112 - val_loss: 0.5450\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.6490 - val_accuracy: 0.8192 - val_loss: 0.5305\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7720 - loss: 0.6376 - val_accuracy: 0.8240 - val_loss: 0.5167\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7802 - loss: 0.6154 - val_accuracy: 0.8262 - val_loss: 0.5076\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7839 - loss: 0.6037 - val_accuracy: 0.8274 - val_loss: 0.5000\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7845 - loss: 0.6019 - val_accuracy: 0.8314 - val_loss: 0.4916\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7856 - loss: 0.5958 - val_accuracy: 0.8324 - val_loss: 0.4871\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7926 - loss: 0.5852 - val_accuracy: 0.8336 - val_loss: 0.4811\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7967 - loss: 0.5732 - val_accuracy: 0.8362 - val_loss: 0.4758\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7968 - loss: 0.5645 - val_accuracy: 0.8384 - val_loss: 0.4710\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8005 - loss: 0.5589 - val_accuracy: 0.8410 - val_loss: 0.4660\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7988 - loss: 0.5592 - val_accuracy: 0.8422 - val_loss: 0.4623\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8023 - loss: 0.5443 - val_accuracy: 0.8420 - val_loss: 0.4602\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8012 - loss: 0.5519 - val_accuracy: 0.8454 - val_loss: 0.4550\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8033 - loss: 0.5481 - val_accuracy: 0.8462 - val_loss: 0.4512\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8033 - loss: 0.5418 - val_accuracy: 0.8460 - val_loss: 0.4484\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8076 - loss: 0.5380 - val_accuracy: 0.8466 - val_loss: 0.4462\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8096 - loss: 0.5315 - val_accuracy: 0.8492 - val_loss: 0.4431\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8079 - loss: 0.5322 - val_accuracy: 0.8486 - val_loss: 0.4415\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8105 - loss: 0.5282 - val_accuracy: 0.8490 - val_loss: 0.4421\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8106 - loss: 0.5238 - val_accuracy: 0.8498 - val_loss: 0.4399\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8127 - loss: 0.5270 - val_accuracy: 0.8512 - val_loss: 0.4363\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8125 - loss: 0.5191 - val_accuracy: 0.8532 - val_loss: 0.4339\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8138 - loss: 0.5188 - val_accuracy: 0.8528 - val_loss: 0.4328\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8156 - loss: 0.5173 - val_accuracy: 0.8536 - val_loss: 0.4290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27b87f707d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No treinamento de redes neurais, momentum e decay são técnicas usadas para melhorar a eficiência e estabilidade da descida do gradiente. Elas ajudam a evitar problemas como mínimos locais e oscilações, além de acelerar a convergência.\n",
    "\n",
    "# 🎢 Momentum\n",
    "O momentum é uma técnica usada para suavizar a atualização dos pesos durante o treinamento. Ele funciona como uma espécie de \"memória\" dos passos anteriores, ajudando a rede a continuar na mesma direção e reduzindo oscilações.\n",
    "\n",
    "Fórmula do Gradient Descent com Momentum:\n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla J(\\theta_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\alpha v_t\n",
    "$$\n",
    "\n",
    "\n",
    " Benefícios do **Momentum**:\n",
    "\n",
    "✅ Acelera a convergência em direções consistentes.\n",
    "\n",
    "✅ Reduz oscilações em direções instáveis.\n",
    "\n",
    "✅ Evita mínimos locais rasos ao manter a \"inércia\" da atualização.\n",
    "\n",
    "📌 **Exemplo intuitivo**: Imagine uma bola rolando por um vale; o momentum faz com que ela continue se movendo mesmo se o gradiente ficar próximo de zero por um tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Decay (Decaimento da Taxa de Aprendizado)\n",
    "O decay (ou \"learning rate decay\") controla a redução gradual da taxa de aprendizado (𝛼) ao longo do treinamento. Isso ajuda a rede a fazer ajustes mais finos nos pesos à medida que se aproxima da solução ótima.\n",
    "\n",
    "Benefícios do **Decay**:\n",
    "\n",
    "✅ Permite passos grandes no início (exploração) e pequenos no fim (convergência precisa).\n",
    "\n",
    "✅ Evita que a rede fique \"presa\" em mínimos locais subótimos.\n",
    "\n",
    "✅ Aumenta a estabilidade do treinamento.\n",
    "\n",
    "📌 **Exemplo intuitivo**: No início, você dá passos largos para encontrar uma solução geral, mas conforme se aproxima da resposta ideal, começa a dar passos menores para refiná-la."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎲 **Resumo**\n",
    "Conceito\t| Objetivo\t| Como funciona\n",
    "-------|----------|---------\n",
    "Momentum\t| Suaviza e acelera a otimização\t| Mantém \"memória\" das atualizações passadas para reduzir oscilações e acelerar a convergência\n",
    "Decay\t| Ajusta a taxa de aprendizado ao longo do tempo\t| Começa com um learning rate alto e o reduz gradualmente para evitar passos muito grandes no final\n",
    "\n",
    "\n",
    "Muitos  otimizadores modernos, como Adam, RMSprop e SGD com Momentum, já integram esses conceitos para melhorar o desempenho do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6204 - loss: 1.0807 - val_accuracy: 0.8242 - val_loss: 0.5056\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7884 - loss: 0.5923 - val_accuracy: 0.8402 - val_loss: 0.4622\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8059 - loss: 0.5380 - val_accuracy: 0.8496 - val_loss: 0.4326\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8149 - loss: 0.5090 - val_accuracy: 0.8532 - val_loss: 0.4186\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8200 - loss: 0.4990 - val_accuracy: 0.8576 - val_loss: 0.4085\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8256 - loss: 0.4857 - val_accuracy: 0.8518 - val_loss: 0.4139\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8280 - loss: 0.4772 - val_accuracy: 0.8594 - val_loss: 0.3957\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8282 - loss: 0.4697 - val_accuracy: 0.8638 - val_loss: 0.3908\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8342 - loss: 0.4534 - val_accuracy: 0.8620 - val_loss: 0.3874\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8346 - loss: 0.4493 - val_accuracy: 0.8640 - val_loss: 0.3803\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8353 - loss: 0.4524 - val_accuracy: 0.8676 - val_loss: 0.3737\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8371 - loss: 0.4451 - val_accuracy: 0.8674 - val_loss: 0.3708\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8354 - loss: 0.4418 - val_accuracy: 0.8718 - val_loss: 0.3672\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8401 - loss: 0.4400 - val_accuracy: 0.8694 - val_loss: 0.3698\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8384 - loss: 0.4398 - val_accuracy: 0.8642 - val_loss: 0.3769\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8425 - loss: 0.4271 - val_accuracy: 0.8734 - val_loss: 0.3582\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8438 - loss: 0.4274 - val_accuracy: 0.8736 - val_loss: 0.3580\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8471 - loss: 0.4171 - val_accuracy: 0.8718 - val_loss: 0.3560\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8420 - loss: 0.4202 - val_accuracy: 0.8714 - val_loss: 0.3536\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8478 - loss: 0.4145 - val_accuracy: 0.8768 - val_loss: 0.3507\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8474 - loss: 0.4162 - val_accuracy: 0.8726 - val_loss: 0.3514\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8520 - loss: 0.4077 - val_accuracy: 0.8746 - val_loss: 0.3498\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8503 - loss: 0.4090 - val_accuracy: 0.8742 - val_loss: 0.3433\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8491 - loss: 0.4079 - val_accuracy: 0.8746 - val_loss: 0.3478\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8511 - loss: 0.4030 - val_accuracy: 0.8782 - val_loss: 0.3415\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8526 - loss: 0.4023 - val_accuracy: 0.8768 - val_loss: 0.3414\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8512 - loss: 0.4015 - val_accuracy: 0.8762 - val_loss: 0.3437\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8552 - loss: 0.3947 - val_accuracy: 0.8742 - val_loss: 0.3427\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8511 - loss: 0.4005 - val_accuracy: 0.8776 - val_loss: 0.3363\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8524 - loss: 0.3971 - val_accuracy: 0.8756 - val_loss: 0.3382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x27b9e79db20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3, momentum=0.9, decay=0.01) ,metrics=['accuracy'])\n",
    "\n",
    "# Momentum = O valor 0.9 indica que 90% do gradiente anterior é mantido para a próxima atualização, e apenas 10% vem do gradiente atual. Ajuda a rede a seguir na direção certa, evitando oscilações.\n",
    "# Decay = 0.01 → Reduz gradativamente a taxa de aprendizado para melhorar a precisão no final.\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
