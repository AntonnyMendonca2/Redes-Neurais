{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturação do **GRADIENTE**\n",
    "\n",
    "Geralmente ocorre em funções de ativação não lineares como Sigmoid e Tangente Hiperbólica (tanh), acontece quando os gradientes se tornam muito pequenos, dificultando o aprendizado da rede\n",
    "\n",
    "✅ Para evitar saturação:\n",
    "\n",
    "- Prefira ReLU, Leaky ReLU ou ELU em vez de sigmoide/tanh.\n",
    "\n",
    "- Use Batch Normalization, que mantém os valores das ativações dentro de uma faixa saudável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuga do **GRADIENTE**\n",
    "\n",
    "Ocorre quando os gradientes se tornam muito grandes, fazendo com que os pesos explodam e a rede fique instável. Esse problema é comum em redes muito profundas, principalmente com funções de ativação que não possuem limite superior. pode acontecer em **Redes Recorrentes**, **Variantes da ReLU (Leaky ReLU, PReLU, ELU)**, funções que não possuem **limite superior para os valores da ativação**.\n",
    "\n",
    "✅ Para evitar fuga do gradiente:\n",
    "\n",
    "- Use Inicialização de Pesos adequada (ex: He Initialization para ReLU).\n",
    "\n",
    "- Utilize Batch Normalization para manter os gradientes sob controle.\n",
    "\n",
    "- Gradiente Clipping: Limita o tamanho do gradiente para evitar explosão.\n",
    "\n",
    "- Regularização L2 (Weight Decay): Penaliza pesos muito grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeakyRelu\n",
    "\n",
    "Utilizando a função de ativação *RelU* (onde o valor de entrada positivo permanece o mesmo, enquanto uma entrada negativa se torna zero), pode ser que o nosso modelo mate alguns neurônios pois se um neurônio recebe valores negativos continuamente, sempre retornará zero. A *LeakyRelu* visa consertar isso. Em sua função, ao invés de zerar valores de entrada negativa, ela os transforma em um fator α pequeno, geralmente α = 0.01, fazendo assim que em algum momento aquele neurônio \"volte a vida\", aprendendo novamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 Como escolher a função de ativação da última camada?\n",
    "\n",
    "| **Tipo de Problema**                                          | **Função de Ativação na Saída**         | **Explicação** |\n",
    "|--------------------------------------------------------------|---------------------------------|--------------|\n",
    "| **Classificação binária** (2 classes: \"sim/não\", \"gato/cachorro\") | `sigmoid`                     | Retorna um valor entre **0 e 1**, interpretado como **probabilidade**. |\n",
    "| **Classificação multiclasse** (3 ou mais classes, previsão de uma única classe) | `softmax`                     | Converte os logits em **probabilidades normalizadas** (somam 1), útil para **1 classe por amostra**. |\n",
    "| **Classificação multirrótulo** (cada amostra pode ter várias classes) | `sigmoid`                      | Cada saída pode ser ativada independentemente, pois cada rótulo é tratado separadamente. |\n",
    "| **Regressão** (previsão de valores contínuos, como temperatura, preço de casa) | `linear` (ou nenhuma ativação) | Permite qualquer valor real como saída, sem restrições. |\n",
    "| **Regressão para valores positivos** (exemplo: previsão de contagem) | `relu` ou `softplus`          | Garante que a saída seja **sempre positiva**. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\OneDrive\\Desktop\\RedesNeurais-Basico\\environment\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "c:\\Users\\anton\\OneDrive\\Desktop\\RedesNeurais-Basico\\environment\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), # Tamanho das imagens\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2), # pode ser \"PReLU()\" no lugar da LeakyReLU, mas se quiser usar SELU precisa definir \"activation='selu'\" e \"kernel_initializer='cun_normal'\"\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax') # 10 pois são 10 classes possíveis de saída. (classes das fotos do MNIST)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.4985 - loss: 1.6399 - val_accuracy: 0.7214 - val_loss: 0.8766\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7320 - loss: 0.8300 - val_accuracy: 0.7750 - val_loss: 0.7001\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.6908 - val_accuracy: 0.8008 - val_loss: 0.6255\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7918 - loss: 0.6270 - val_accuracy: 0.8134 - val_loss: 0.5816\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8037 - loss: 0.5867 - val_accuracy: 0.8194 - val_loss: 0.5498\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8117 - loss: 0.5566 - val_accuracy: 0.8236 - val_loss: 0.5298\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8198 - loss: 0.5366 - val_accuracy: 0.8326 - val_loss: 0.5085\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8199 - loss: 0.5282 - val_accuracy: 0.8348 - val_loss: 0.4951\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8279 - loss: 0.5045 - val_accuracy: 0.8372 - val_loss: 0.4841\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8325 - loss: 0.4932 - val_accuracy: 0.8446 - val_loss: 0.4718\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8319 - loss: 0.4865 - val_accuracy: 0.8442 - val_loss: 0.4663\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8321 - loss: 0.4819 - val_accuracy: 0.8478 - val_loss: 0.4570\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8367 - loss: 0.4720 - val_accuracy: 0.8480 - val_loss: 0.4544\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8408 - loss: 0.4584 - val_accuracy: 0.8500 - val_loss: 0.4479\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8404 - loss: 0.4586 - val_accuracy: 0.8522 - val_loss: 0.4405\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8426 - loss: 0.4534 - val_accuracy: 0.8548 - val_loss: 0.4370\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8428 - loss: 0.4473 - val_accuracy: 0.8552 - val_loss: 0.4327\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8456 - loss: 0.4426 - val_accuracy: 0.8554 - val_loss: 0.4274\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8463 - loss: 0.4424 - val_accuracy: 0.8556 - val_loss: 0.4278\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8488 - loss: 0.4391 - val_accuracy: 0.8548 - val_loss: 0.4229\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8494 - loss: 0.4360 - val_accuracy: 0.8600 - val_loss: 0.4183\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8524 - loss: 0.4264 - val_accuracy: 0.8594 - val_loss: 0.4210\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8536 - loss: 0.4223 - val_accuracy: 0.8618 - val_loss: 0.4119\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8557 - loss: 0.4169 - val_accuracy: 0.8610 - val_loss: 0.4102\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8535 - loss: 0.4220 - val_accuracy: 0.8620 - val_loss: 0.4092\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8573 - loss: 0.4101 - val_accuracy: 0.8612 - val_loss: 0.4090\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8577 - loss: 0.4130 - val_accuracy: 0.8640 - val_loss: 0.4037\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8601 - loss: 0.4050 - val_accuracy: 0.8638 - val_loss: 0.4067\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8572 - loss: 0.4074 - val_accuracy: 0.8644 - val_loss: 0.4031\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8590 - loss: 0.4049 - val_accuracy: 0.8644 - val_loss: 0.4023\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❌ Piora no modelo\n",
    "\n",
    "Por que será que o modelo utilizando o leakyRelU piorou em relação ao que fizemos no arquivo \"RedesSequenciais.ipynb\" utilizando uma rede sequencial simples?\n",
    "\n",
    "\n",
    "\n",
    "Fator |\tModelo com LeakyReLU (pior desempenho) |\tModelo com ReLU (melhor desempenho)\n",
    "|------------------------------------------------------|---------------------------------|--------------|\n",
    "Função de ativação |\tLeakyReLU(α=0.2) (mantém gradientes negativos, pode atrapalhar) |\tReLU (aprendizado mais rápido e eficiente)\n",
    "Inicialização |\the_normal (pode gerar pesos grandes demais) |\tglorot_uniform (mais equilibrado para redes pequenas)\n",
    "Complexidade |\tMais camadas e inicializações desnecessárias |\tModelo mais simples e eficiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎲 batch Normalization\n",
    "\n",
    "Utilizada para **normalizar** as **ativações das camadas intermediárias** de uma rede neural, o **BathNormalization** acelera o treinamento, melhora a performance e reduz a sensibilidade a inicialização dos pesos.\n",
    "\n",
    "Quando treinamos uma rede neural, as ativações das camadas podem ficar em diferentes escalas, o que pode dificultar a cnovergência durante o treinamento, já que o gradiente pode se tornar muito grande ou muito pequeno. Isso é conhecido como **covariate shift** (Mudança na distribuição das ativações das camadas). O **BatchNormalization** garante que as ativações tenham distribuição com média **0** e desvio padrão **1** durante o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anton\\OneDrive\\Desktop\\RedesNeurais-Basico\\environment\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1e-3) ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5779 - loss: 1.2571 - val_accuracy: 0.7992 - val_loss: 0.5991\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7826 - loss: 0.6293 - val_accuracy: 0.8222 - val_loss: 0.5197\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8041 - loss: 0.5611 - val_accuracy: 0.8398 - val_loss: 0.4831\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8225 - loss: 0.5148 - val_accuracy: 0.8478 - val_loss: 0.4597\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8283 - loss: 0.4919 - val_accuracy: 0.8510 - val_loss: 0.4432\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8347 - loss: 0.4786 - val_accuracy: 0.8538 - val_loss: 0.4326\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8389 - loss: 0.4596 - val_accuracy: 0.8580 - val_loss: 0.4213\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8375 - loss: 0.4600 - val_accuracy: 0.8594 - val_loss: 0.4158\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8475 - loss: 0.4416 - val_accuracy: 0.8596 - val_loss: 0.4066\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8480 - loss: 0.4327 - val_accuracy: 0.8604 - val_loss: 0.4022\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8489 - loss: 0.4238 - val_accuracy: 0.8616 - val_loss: 0.3968\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8535 - loss: 0.4140 - val_accuracy: 0.8646 - val_loss: 0.3930\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8519 - loss: 0.4153 - val_accuracy: 0.8640 - val_loss: 0.3879\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8557 - loss: 0.4108 - val_accuracy: 0.8654 - val_loss: 0.3857\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8581 - loss: 0.4039 - val_accuracy: 0.8674 - val_loss: 0.3822\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3911 - val_accuracy: 0.8676 - val_loss: 0.3782\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8616 - loss: 0.3923 - val_accuracy: 0.8684 - val_loss: 0.3748\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8628 - loss: 0.3870 - val_accuracy: 0.8698 - val_loss: 0.3733\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8643 - loss: 0.3814 - val_accuracy: 0.8694 - val_loss: 0.3705\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8674 - loss: 0.3770 - val_accuracy: 0.8706 - val_loss: 0.3686\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8691 - loss: 0.3693 - val_accuracy: 0.8714 - val_loss: 0.3667\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8692 - loss: 0.3715 - val_accuracy: 0.8718 - val_loss: 0.3652\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8668 - loss: 0.3755 - val_accuracy: 0.8734 - val_loss: 0.3619\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8698 - loss: 0.3632 - val_accuracy: 0.8720 - val_loss: 0.3608\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8717 - loss: 0.3635 - val_accuracy: 0.8732 - val_loss: 0.3584\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8740 - loss: 0.3589 - val_accuracy: 0.8732 - val_loss: 0.3576\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8757 - loss: 0.3534 - val_accuracy: 0.8722 - val_loss: 0.3551\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8735 - loss: 0.3598 - val_accuracy: 0.8754 - val_loss: 0.3523\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8761 - loss: 0.3526 - val_accuracy: 0.8748 - val_loss: 0.3520\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8761 - loss: 0.3473 - val_accuracy: 0.8746 - val_loss: 0.3505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bbae2def60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
